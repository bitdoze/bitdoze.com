---
date: 2026-02-10T00:00:00Z
title: "OpenClaw Alternatives Worth Trying in 2026"
description: "A look at NanoClaw, nanobot, memU, and bitdoze-bot as self-hosted alternatives to OpenClaw for running your own 24/7 AI assistant."
image: "../../assets/images/26/02/openclaw-alternatives.svg"
categories: ["AI"]
authors: ["Dragos"]
tags: ["ai-tools", "self-hosted"]
canonical: "https://www.bitdoze.com/openclaw-alternatives/"
---

import Notice from "@components/widgets/Notice.astro";
import Button from "@components/widgets/Button.astro";
import ListCheck from "@components/widgets/ListCheck.astro";
import Accordion from "@components/widgets/Accordion.astro";
import YouTubeEmbed from "@components/widgets/YouTubeEmbed.astro";
import Tabs from "@components/widgets/Tabs.astro";
import Tab from "@components/widgets/Tab.astro";

OpenClaw (the project that went through the Clawdbot and Moltbot name changes) made a lot of people realize they could run an AI assistant on their own server. Always on, always reachable through Telegram or Slack, and not dependent on anyone's SaaS. I've been running it myself and wrote a [full setup guide](/clawdbot-setup-guide/) if you want to try the original.

But OpenClaw isn't the only option anymore. Several projects have appeared with different takes on the same idea. Some are smaller and more focused. Some try to do more. I've been looking at four of them, and each one makes different tradeoffs worth knowing about.

<Notice type="info" title="What this covers">
Four self-hosted AI bot projects that work as OpenClaw alternatives. Each section includes what the project does, how it's different, and how to get it running.
</Notice>

## Quick comparison

Before getting into each project, here's how they stack up:

| | NanoClaw | nanobot | memU | bitdoze-bot |
|---|---|---|---|---|
| **GitHub stars** | ~10 | 15.4k | 8.7k | 7 |
| **Language** | Python | Python | Python + Rust | Python |
| **Codebase size** | ~3k lines | ~3.5k lines | Larger (framework) | Medium |
| **License** | MIT | MIT | Apache 2.0 | MIT |
| **Chat channels** | Telegram | Telegram, Discord, WhatsApp, Slack, Feishu, DingTalk, Email, QQ | Bot at memu.bot | Discord |
| **Memory** | SQLite | Built-in | Hierarchical (main feature) | Agno memory + learning |
| **Install method** | Docker / manual | pip (`nanobot-ai`) | pip (`memu-py`) | UV + manual |
| **Local models** | Via OpenRouter | vLLM support | Via custom providers | OpenAI-compatible |
| **Security features** | FileGuard, ShellSandbox, PromptGuard | Basic | N/A (framework) | Tool permissions, audit |
| **Multi-agent** | No | No | No (memory layer) | Yes (Agno teams) |

## 1. NanoClaw

<Button text="GitHub Repository" link="https://github.com/ysz/nanoClaw" variant="solid" color="green" size="md" icon="github" />

NanoClaw is the smallest project on this list. Around 3,000 lines of Python, MIT license, Telegram-only. The author built it as a stripped-down OpenClaw that you can actually read through in an afternoon.

What caught my attention is the security layer. Most of these bots give the AI some form of file and shell access, and NanoClaw is the only one that built explicit guards around that from the start.

### What it does

- Telegram bot that responds to messages and can use tools (file access, shell commands, web search)
- SQLite-based memory that persists across restarts
- Web dashboard for monitoring conversations and memory
- Model-agnostic through OpenRouter, so you pick whatever LLM you want
- Docker support for deployment

### Security approach

NanoClaw has three built-in safety mechanisms:

- **FileGuard**: Restricts which directories the AI can read from and write to. You define allowed paths in config.
- **ShellSandbox**: Shell commands run in a restricted environment. No `rm -rf /` situations.
- **PromptGuard**: Filters for prompt injection attempts before they reach the model.

None of these are bulletproof, but having them at all puts NanoClaw ahead of projects that just hand the AI a shell and hope for the best.

### Getting started

<Tabs>
<Tab name="Docker">

```bash
git clone https://github.com/ysz/nanoClaw.git
cd nanoClaw
cp .env.example .env
# Edit .env with your Telegram bot token and OpenRouter API key
docker compose up -d
```

</Tab>
<Tab name="Manual">

```bash
git clone https://github.com/ysz/nanoClaw.git
cd nanoClaw
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your tokens
python main.py
```

</Tab>
</Tabs>

### Who this is for

If you want something small enough to understand completely, and you only need Telegram, NanoClaw is a solid pick. The security features make it less nerve-wracking to give the AI tool access. The downside is the tiny community (10 stars as of writing) and Telegram-only support.

## 2. nanobot

<Button text="GitHub Repository" link="https://github.com/HKUDS/nanobot" variant="solid" color="blue" size="md" icon="github" />

nanobot comes from HKUDS (Hong Kong University) and has grown fast. 15,400 stars, 2,200 forks. The pitch is similar to NanoClaw but much more ambitious: a lightweight bot (~3,500 lines of code) that connects to basically every chat platform.

### What it does

- Connects to Telegram, Discord, WhatsApp, Slack, Feishu, DingTalk, Email, and QQ
- Installable via pip: `pip install nanobot-ai`
- Built-in memory system
- Supports local models through vLLM
- Works with OpenRouter, Anthropic, OpenAI, DeepSeek, Groq, Gemini, and others
- Docker deployment available

The channel coverage is what sets nanobot apart. If you need your bot on WhatsApp and Discord and Slack at the same time, most alternatives don't do that without significant extra work.

### Setup

```bash
pip install nanobot-ai
nanobot init
# Follow the wizard to configure channels and API keys
nanobot start
```

That's really it for a basic setup. The `init` wizard walks you through picking a chat platform and connecting an LLM provider. You can add more channels later.

### Local model support

nanobot can connect to a local vLLM server, which means you can run the whole stack without any API costs after the initial hardware investment. If you already have an Ollama or vLLM setup, pointing nanobot at it is straightforward.

```bash
# Example: using a local vLLM endpoint
nanobot config set llm.base_url http://localhost:8000/v1
nanobot config set llm.model your-local-model
```

### Who this is for

nanobot is the pragmatic choice if you need multi-channel support or want something you can install with pip and have running in five minutes. The university backing and large community mean bugs get fixed and features get added regularly. The tradeoff is that with so many integrations, the configuration can get dense.

## 3. memU

<Button text="GitHub Repository" link="https://github.com/NevaMind-AI/MemU" variant="solid" color="purple" size="md" icon="github" />

memU is different from the other projects here. It's not really a chatbot. It's a memory framework built for 24/7 agents, and it happens to include a bot (at memu.bot) as a reference implementation.

The core idea: most chatbots forget everything between sessions, and even ones with memory just do basic retrieval. memU treats memory like a file system, with categories, items, and cross-references, and it tries to predict what you're about to need before you ask for it.

### How the memory works

memU organizes everything into three layers:

| Layer | What it stores | Purpose |
|---|---|---|
| Resources | Raw conversations, documents, images | Original data |
| Items | Extracted facts, preferences, skills | Searchable knowledge |
| Categories | Auto-organized topics | Navigation and context |

The "file system" metaphor means your agent's memory looks like this:

```
memory/
├── preferences/
│   ├── communication_style.md
│   └── topic_interests.md
├── knowledge/
│   ├── domain_expertise/
│   └── learned_skills/
└── context/
    ├── recent_conversations/
    └── pending_tasks/
```

New memories get auto-categorized. Related memories link to each other. The system claims 92% accuracy on the Locomo benchmark, which tests how well memory systems retain and retrieve information over long conversations.

### Proactive behavior

This is where memU gets interesting. Instead of just answering when asked, it monitors conversations and tries to anticipate what you'll need next. The agent can:

- Pre-fetch relevant context before you explicitly ask
- Notice patterns in what you're working on and surface related memories
- Draft action items from conversation flow
- Learn your preferences over time and adjust responses

Whether this is useful or annoying probably depends on your tolerance for unsolicited suggestions. I can see it working well for someone who uses an AI assistant all day, less so for occasional use.

### Setup

<Tabs>
<Tab name="Cloud">

The hosted version at [memu.so](https://memu.so) runs continuously. If you want to try the memory system without self-hosting, this is the fastest path.

</Tab>
<Tab name="Self-hosted">

```bash
pip install memu-py

# In-memory test (no database needed)
export OPENAI_API_KEY=your_key
cd tests
python test_inmemory.py

# With PostgreSQL for persistent storage
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

python test_postgres.py
```

</Tab>
</Tabs>

memU also supports OpenRouter, so you can route through whatever model provider you prefer:

```python
from memu import MemoryService

service = MemoryService(
    llm_profiles={
        "default": {
            "provider": "openrouter",
            "base_url": "https://openrouter.ai",
            "api_key": "your_openrouter_api_key",
            "chat_model": "anthropic/claude-3.5-sonnet",
        },
    },
)
```

### Who this is for

memU makes the most sense if you're building your own agent and want a proper memory layer underneath it. It's also worth looking at if you're frustrated with how shallow the memory is in other bots. The project has 8,700 stars and an active community. The downside is the complexity. This isn't a "clone and run" bot like NanoClaw. It's a framework, and you'll need to integrate it into something.

## 4. bitdoze-bot

<Button text="GitHub Repository" link="https://github.com/bitdoze/bitdoze_bot" variant="solid" color="red" size="md" icon="github" />

This is my project. I built it because I wanted a Discord bot that could handle multiple specialized agents working together, not just one model answering questions.

bitdoze-bot uses the [Agno framework](/agno-get-start/) for multi-agent orchestration. You define agent "teams" in workspace folders, each agent with its own tools and personality, and a coordinator routes incoming messages to the right specialist.

I wrote a detailed build guide for it: [Build your own AI Discord bot with Agno teams](/create-your-own-ai-agent/).

<YouTubeEmbed
  url="https://www.youtube.com/embed/yoWGFO7tvpc"
  label="I Built a Personal AI Assistant in 90 Minutes"
/>

### What it does

- Discord-first (responds on mention)
- Multi-agent teams: a coordinator + specialist agents (coding, research, ops, whatever you define)
- Workspace-based config: each agent gets its own folder with instructions, tools, and permissions
- Memory and learning: stores context across conversations and learns from corrections
- Heartbeat + cron: scheduled health checks and recurring tasks
- Tool permissions and audit logging
- Observability: structured logs for every agent run

### The team setup

The multi-agent approach is the main difference from everything else on this list. Instead of one model trying to do everything, you split responsibilities:

```
workspaces/
├── main/
│   └── agent.yaml          # Coordinator - routes to specialists
├── coding/
│   └── agent.yaml          # Coding specialist
├── research/
│   └── agent.yaml          # Web research specialist
└── ops/
    └── agent.yaml          # Server ops specialist
```

When a message comes in, the coordinator decides which specialist handles it. You can start with just the `main` agent and add specialists as you need them.

### Getting started

```bash
git clone https://github.com/bitdoze/bitdoze_bot.git
cd bitdoze_bot
cp .env.example .env
# Edit .env with Discord token and API keys

# Install with UV
uv sync
uv run python main.py
```

You need Python 3.12+, a Discord bot token, and an API key for your model provider.

### Who this is for

If you want to go beyond a single-agent chatbot and experiment with multi-agent coordination, this is the project to look at. The workspace-based configuration makes it easy to add new specialists without touching the core code. The tradeoff is that it's Discord-only and the smallest project here by star count. But it's the one I use daily, and the multi-agent approach has been worth the extra setup.

## Which one should you pick?

It depends on what you actually need:

<ListCheck>
<ul>
<li>Want a tiny, readable, security-focused Telegram bot? Go with NanoClaw.</li>
<li>Need to be on Telegram, Discord, WhatsApp, and Slack at once? nanobot handles that.</li>
<li>Building your own agent and need a real memory system? memU is the memory framework to look at.</li>
<li>Want multi-agent teams on Discord? bitdoze-bot does that with Agno.</li>
</ul>
</ListCheck>

All four run on a basic VPS. A Hetzner CX22 ($5.50/month) is enough for any of them. API costs depend on which model you pick and how much you chat, but $15-50/month covers most people.

If you haven't tried any self-hosted AI bot yet, I'd actually recommend starting with [OpenClaw itself](/clawdbot-setup-guide/). It's the most documented, has the largest community, and the setup wizard makes the first run pretty painless. Once you know what you want to change about it, these alternatives start making more sense.

<Accordion label="Frequently asked questions" group="faq" expanded="true">

**Can I switch from OpenClaw to one of these without losing my conversations?**

Not directly. Each project stores memory differently. You'd need to export from OpenClaw and manually import, or just start fresh. memU has the most flexible import options since it's designed as a memory framework.

**Do any of these work on a Raspberry Pi?**

NanoClaw and nanobot can run on a Pi 4 with 4GB+ RAM. Performance will be limited. memU with PostgreSQL needs more resources. bitdoze-bot depends on how many agents you run.

**Can I use local models instead of API providers?**

nanobot has native vLLM support. NanoClaw and bitdoze-bot work with any OpenAI-compatible endpoint, so you can point them at Ollama or vLLM. memU supports custom LLM providers. See our [Ollama Docker guide](/ollama-docker-install/) for setting up local models.

**How much coding do these require?**

NanoClaw and nanobot are close to zero-code for basic setups. bitdoze-bot needs some YAML configuration for agents. memU requires Python integration work since it's a framework, not a standalone bot.

</Accordion>
