---
date: 2026-02-10T00:00:00Z
title: "OpenClaw Alternatives Worth Trying in 2026"
description: "A look at NanoClaw, nanobot, memU, bitdoze-bot, PicoClaw, IronClaw, and ZeroClaw as self-hosted alternatives to OpenClaw for running your own 24/7 AI assistant."
image: "../../assets/images/26/02/openclaw-alternatives.svg"
categories: ["AI"]
authors: ["Dragos"]
tags: ["ai-tools", "self-hosted"]
canonical: "https://www.bitdoze.com/openclaw-alternatives/"
---

import Notice from "@components/widgets/Notice.astro";
import Button from "@components/widgets/Button.astro";
import ListCheck from "@components/widgets/ListCheck.astro";
import Accordion from "@components/widgets/Accordion.astro";
import YouTubeEmbed from "@components/widgets/YouTubeEmbed.astro";
import Tabs from "@components/widgets/Tabs.astro";
import Tab from "@components/widgets/Tab.astro";

OpenClaw (the project that went through Clawdbot and Moltbot name changes) made a lot of people realize they could run an AI assistant on their own server. Always on, always reachable through Telegram or Slack, and not dependent on anyone's SaaS. I've been running it myself and wrote a [full setup guide](/clawdbot-setup-guide/) if you want to try the original.

But OpenClaw isn't the only option anymore. Several projects have appeared with different takes on the same idea. Some are smaller and more focused. Some try to do more. I've been looking at four of them, and each one makes different tradeoffs worth knowing about.

<Notice type="info" title="What this covers">
Six self-hosted AI bot projects that work as OpenClaw alternatives. Each section includes what the project does, how it's different, and how to get it running.
</Notice>

<Notice type="info" title="New addition">
ZeroClaw was added to this roundup on February 16, 2026. It's a Rust-based assistant with a 3.4MB binary and under 5MB RAM usage. See section 7 or our [full ZeroClaw setup guide](/zeroclaw-setup-guide/).
</Notice>

## Quick comparison

Before getting into each project, here's how they stack up:

| Feature | NanoClaw | nanobot | memU | bitdoze-bot | PicoClaw | IronClaw | ZeroClaw |
|---|---|---|---|---|---|---|---|
| **GitHub stars** | 9.3k | 21.6k | 9.6k | 10 | 16k | 2.4k | 14.2k |
| **Language** | Python | Python | Python + Rust | Python | Go | Rust | Rust |
| **Codebase size** | ~3k lines | ~3.5k lines | Larger (framework) | Medium | Small (single binary) | Medium-large | Medium (1,017 tests) |
| **License** | MIT | MIT | Apache 2.0 | MIT | MIT | Apache 2.0 / MIT | MIT |
| **Chat channels** | Telegram | Telegram, Discord, WhatsApp, Slack, Feishu, DingTalk, Email, QQ | Bot at memu.bot | Discord | Telegram, Discord | REPL, HTTP, Telegram, Slack (WASM) | CLI, Telegram, Discord, Slack, iMessage, Matrix, WhatsApp, Webhook |
| **Memory** | SQLite | Built-in | Hierarchical (main feature) | Agno memory + learning | File-based workspace | PostgreSQL + pgvector (hybrid search) | SQLite hybrid (FTS5 + vector cosine) |
| **Install method** | Docker / manual | pip (`nanobot-ai`) | pip (`memu-py`) | UV + manual | Single binary / source | cargo build | cargo build / Docker |
| **Local models** | Via OpenRouter | vLLM support | Via custom providers | OpenAI-compatible | Via OpenRouter | Via NEAR AI | Ollama + 22 providers |
| **Security features** | FileGuard, ShellSandbox, PromptGuard | Basic | N/A (framework) | Tool permissions, audit | Basic | WASM sandbox, credential protection, prompt injection defense | Gateway pairing, sandbox, allowlists, encrypted secrets |
| **Multi-agent** | No | No | No (memory layer) | Yes (Agno teams) | No | Parallel jobs with isolated workers | No |

## 1. NanoClaw

<Button text="GitHub Repository" link="https://github.com/qwibitai/nanoclaw" variant="solid" color="green" size="md" icon="github" />

NanoClaw is the smallest project on this list. Around 3,000 lines of Python, MIT license, Telegram-only. The author built it as a stripped-down OpenClaw that you can actually read through in an afternoon.

What caught my attention is the security layer. Most of these bots give the AI some form of file and shell access, and NanoClaw is the only one that built explicit guards around that from the start.

### What it does

- Telegram bot that responds to messages and can use tools (file access, shell commands, web search)
- SQLite-based memory that persists across restarts
- Web dashboard for monitoring conversations and memory
- Model-agnostic through OpenRouter, so you pick whatever LLM you want
- Docker support for deployment

### Security approach

NanoClaw has three built-in safety mechanisms:

- **FileGuard**: Restricts which directories the AI can read from and write to. You define allowed paths in config.
- **ShellSandbox**: Shell commands run in a restricted environment. No `rm -rf /` situations.
- **PromptGuard**: Filters for prompt injection attempts before they reach the model.

None of these are bulletproof, but having them at all puts NanoClaw ahead of projects that just hand the AI a shell and hope for the best.

### Getting started

<Tabs>
<Tab name="Docker">

```bash
git clone https://github.com/ysz/nanoClaw.git
cd nanoClaw
cp .env.example .env
# Edit .env with your Telegram bot token and OpenRouter API key
docker compose up -d
```

</Tab>
<Tab name="Manual">

```bash
git clone https://github.com/ysz/nanoClaw.git
cd nanoClaw
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your tokens
python main.py
```

</Tab>
</Tabs>

### Who this is for

If you want something small enough to understand completely, and you only need Telegram, NanoClaw is a solid pick. The security features make it less nerve-wracking to give the AI tool access. The downside is a tiny community (10 stars as of writing) and Telegram-only support.

## 2. nanobot

<Button text="GitHub Repository" link="https://github.com/HKUDS/nanobot" variant="solid" color="blue" size="md" icon="github" />

nanobot comes from HKUDS (Hong Kong University) and has grown fast. 15,400 stars, 2,200 forks. The pitch is similar to NanoClaw but much more ambitious: a lightweight bot (~3,500 lines of code) that connects to basically every chat platform. We wrote a [full nanobot setup guide](/nanobot-setup-guide/) covering MiniMax M2.5, GLM-5, and Brave Search if you want to try it.

### What it does

- Connects to Telegram, Discord, WhatsApp, Slack, Feishu, DingTalk, Email, and QQ
- Installable via pip: `pip install nanobot-ai`
- Built-in memory system
- Supports local models through vLLM
- Works with OpenRouter, Anthropic, OpenAI, DeepSeek, Groq, Gemini, and others
- Docker deployment available

The channel coverage is what sets nanobot apart. If you need your bot on WhatsApp and Discord and Slack at the same time, most alternatives don't do that without significant extra work.

### Setup

```bash
pip install nanobot-ai
nanobot init
# Follow wizard to configure channels and API keys
nanobot start
```

That's really it for a basic setup. The `init` wizard walks you through picking a chat platform and connecting an LLM provider. You can add more channels later.

### Local model support

nanobot can connect to a local vLLM server, which means you can run the whole stack without any API costs after the initial hardware investment. If you already have an Ollama or vLLM setup, pointing nanobot at it is straightforward.

```bash
# Example: using a local vLLM endpoint
nanobot config set llm.base_url http://localhost:8000/v1
nanobot config set llm.model your-local-model
```

### Who this is for

nanobot is the pragmatic choice if you need multi-channel support or want something you can install with pip and have running in five minutes. The university backing and large community mean bugs get fixed and features get added regularly. The tradeoff is that with so many integrations, configuration can get dense.

## 3. memU

<Button text="GitHub Repository" link="https://github.com/NevaMind-AI/MemU" variant="solid" color="purple" size="md" icon="github" />

memU is different from the other projects here. It's not really a chatbot. It's a memory framework built for 24/7 agents, and it happens to include a bot (at memu.bot) as a reference implementation.

The core idea: most chatbots forget everything between sessions, and even ones with memory just do basic retrieval. memU treats memory like a file system, with categories, items, and cross-references, and it tries to predict what you're about to need before you ask for it.

### How memory works

memU organizes everything into three layers:

| Layer | What it stores | Purpose |
|---|---|---|
| Resources | Raw conversations, documents, images | Original data |
| Items | Extracted facts, preferences, skills | Searchable knowledge |
| Categories | Auto-organized topics | Navigation and context |

The "file system" metaphor means your agent's memory looks like this:

```
memory/
├── preferences/
│   ├── communication_style.md
│   └── topic_interests.md
├── knowledge/
│   ├── domain_expertise/
│   └── learned_skills/
└── context/
    ├── recent_conversations/
    └── pending_tasks/
```

New memories get auto-categorized. Related memories link to each other. The system claims 92% accuracy on the Locomo benchmark, which tests how well memory systems retain and retrieve information over long conversations.

### Proactive behavior

This is where memU gets interesting. Instead of just answering when asked, it monitors conversations and tries to anticipate what you'll need next. The agent can:

- Pre-fetch relevant context before you explicitly ask
- Notice patterns in what you're working on and surface related memories
- Draft action items from conversation flow
- Learn your preferences over time and adjust responses

Whether this is useful or annoying probably depends on your tolerance for unsolicited suggestions. I can see it working well for someone who uses an AI assistant all day, less so for occasional use.

### Setup

<Tabs>
<Tab name="Cloud">

The hosted version at [memu.so](https://memu.so) runs continuously. If you want to try a memory system without self-hosting, this is the fastest path.

</Tab>
<Tab name="Self-hosted">

```bash
pip install memu-py

# In-memory test (no database needed)
export OPENAI_API_KEY=your_key
cd tests
python test_inmemory.py

# With PostgreSQL for persistent storage
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

python test_postgres.py
```

</Tab>
</Tabs>

memU also supports OpenRouter, so you can route through whatever model provider you prefer:

```python
from memu import MemoryService

service = MemoryService(
    llm_profiles={
        "default": {
            "provider": "openrouter",
            "base_url": "https://openrouter.ai",
            "api_key": "your_openrouter_api_key",
            "chat_model": "anthropic/claude-3.5-sonnet",
        },
    },
)
```

### Who this is for

memU makes the most sense if you're building your own agent and want a proper memory layer underneath it. It's also worth looking at if you're frustrated with how shallow memory is in other bots. The project has 8,700 stars and an active community. The downside is complexity. This isn't a "clone and run" bot like NanoClaw. It's a framework, and you'll need to integrate it into something.

## 4. bitdoze-bot

<Button text="GitHub Repository" link="https://github.com/bitdoze/bitdoze_bot" variant="solid" color="red" size="md" icon="github" />

This is my project. I built it because I wanted a Discord bot that could handle multiple specialized agents working together, not just one model answering questions.

bitdoze-bot uses the [Agno framework](/agno-get-start/) for multi-agent orchestration. You define agent "teams" in workspace folders, each agent with its own tools and personality, and a coordinator routes incoming messages to the right specialist.

I wrote a detailed build guide for it: [Build your own AI Discord bot with Agno teams](/create-your-own-ai-agent/).

<YouTubeEmbed
  url="https://www.youtube.com/embed/yoWGFO7tvpc"
  label="I Built a Personal AI Assistant in 90 Minutes"
/>

### What it does

- Discord-first (responds on mention)
- Multi-agent teams: a coordinator + specialist agents (coding, research, ops, whatever you define)
- Workspace-based config: each agent gets its own folder with instructions, tools, and permissions
- Memory and learning: stores context across conversations and learns from corrections
- Heartbeat + cron: scheduled health checks and recurring tasks
- Tool permissions and audit logging
- Observability: structured logs for every agent run

### The team setup

The multi-agent approach is the main difference from everything else on this list. Instead of one model trying to do everything, you split responsibilities:

```
workspaces/
├── main/
│   └── agent.yaml          # Coordinator - routes to specialists
├── coding/
│   └── agent.yaml          # Coding specialist
├── research/
│   └── agent.yaml          # Web research specialist
└── ops/
    └── agent.yaml          # Server ops specialist
```

When a message comes in, the coordinator decides which specialist handles it. You can start with just the `main` agent and add specialists as you need them.

### Getting started

```bash
git clone https://github.com/bitdoze/bitdoze_bot.git
cd bitdoze_bot
cp .env.example .env
# Edit .env with Discord token and API keys

# Install with UV
uv sync
uv run python main.py
```

You need Python 3.12+, a Discord bot token, and an API key for your model provider.

### Who this is for

If you want to go beyond a single-agent chatbot and experiment with multi-agent coordination, this is the project to look at. The workspace-based configuration makes it easy to add new specialists without touching the core code. The tradeoff is that it's Discord-only and the smallest project here by star count. But it's one I use daily, and the multi-agent approach has been worth the extra setup.

## 5. PicoClaw

<Button text="GitHub Repository" link="https://github.com/sipeed/picoclaw" variant="solid" color="gray" size="md" icon="github" />

PicoClaw takes the opposite approach from everything else on this list. Instead of adding features, it strips them away. The project is a Go rewrite of nanobot that compiles to a single binary, uses less than 10MB of RAM, and boots in under a second. Sipeed (the RISC-V hardware company) built it to run on their $10 LicheeRV-Nano boards, which says a lot about the resource budget they were working with. We wrote a [full PicoClaw setup guide](/picoclaw-setup-guide/) covering MiniMax M2.5, GLM-5, and Discord if you want to try it.

The whole thing was reportedly written in a single day, with the AI agent itself driving most of the Go migration. That sounds like a gimmick, but the result actually works. The binary runs on RISC-V, ARM, and x86 without changes.

### What it does

- Single binary AI assistant, no runtime dependencies
- Telegram and Discord support
- Tool access: shell commands, file operations, web search (via Brave Search API)
- Works with OpenRouter, Zhipu, Anthropic, OpenAI, Gemini, Groq, and DeepSeek
- Workspace-based file storage for memory and logs
- CLI mode for local use, gateway mode for chat channels
- Voice message transcription through Groq's Whisper

### Resource comparison

The numbers here are hard to ignore:

| Metric | OpenClaw | nanobot | PicoClaw |
|---|---|---|---|
| **RAM** | >1GB | >100MB | under 10MB |
| **Startup (0.8GHz core)** | >500s | >30s | under 1s |
| **Minimum hardware cost** | Mac Mini $599 | ~$50 SBC | ~$10 board |

If you have a NanoKVM or MaixCAM sitting around, PicoClaw can turn it into an always-on assistant. That's a use case none of the other projects here can touch.

### Getting started

```bash
# Download prebuilt binary from releases, or build from source:
git clone https://github.com/sipeed/picoclaw.git
cd picoclaw
make build

# Initialize config
picoclaw onboard

# Edit ~/.picoclaw/config.json with your API keys

# Chat directly
picoclaw agent -m "What is 2+2?"

# Or start as gateway for Telegram/Discord
picoclaw gateway
```

### Who this is for

PicoClaw is the pick if you care about resource efficiency above all else, or if you want to run a bot on hardware that would choke on Python. The Go codebase is small and readable, and the single-binary deployment means there's nothing to install. The tradeoff is that it's brand new (launched February 2026), so the feature set is more limited than nanobot, and the community is still forming. But 1,100 stars in a few days suggests people are paying attention.

## 6. IronClaw

<Button text="GitHub Repository" link="https://github.com/nearai/ironclaw" variant="solid" color="blue" size="md" icon="github" />
<Button text="ZeroClaw Fork" link="https://github.com/theonlyhennygod/zeroclaw" variant="solid" color="blue" size="md" icon="github" />

IronClaw comes from NEAR AI and takes a security-first approach that goes well beyond what the other projects attempt. It's a full Rust rewrite of the OpenClaw concept, and the main selling point is the WASM sandbox. Every untrusted tool runs inside an isolated WebAssembly container with explicit capability-based permissions. Your API keys never get exposed to tool code. HTTP requests only go to hosts you've approved.

If you've ever felt nervous about giving an AI agent shell access on a box with real data on it, IronClaw was built for that anxiety.

### What it does

- Rust-native AI assistant with REPL, HTTP webhook, and WASM-based channels (Telegram, Slack)
- WASM sandbox for all tool execution with capability-based permissions
- Credential protection: secrets get injected at the host boundary, tool code never sees them
- Prompt injection defense with pattern detection and content sanitization
- Endpoint allowlisting so the agent can only reach hosts you approve
- PostgreSQL with pgvector for memory (full-text + vector hybrid search)
- Heartbeat system for proactive background tasks
- Parallel job execution with isolated contexts
- Self-expanding: describe a tool you need, and IronClaw builds it as a WASM module
- MCP protocol support for connecting external tool servers

### The security model

This is where IronClaw stands apart. The security pipeline for tool execution looks like this:

1. Tool request hits the endpoint allowlist validator
2. Request gets scanned for credential leaks
3. Credentials get injected at the host boundary (tool code never holds them)
4. Request executes inside the WASM sandbox
5. Response gets scanned again for credential leaks
6. Result returns to the agent

There are also per-tool rate limits and resource caps (memory, CPU time, execution duration). Policy rules let you set severity levels for different situations: block, warn, review, or sanitize.

No other project on this list has anything close to this level of isolation. NanoClaw has its FileGuard and ShellSandbox, but those are Python-level guards. IronClaw's WASM containers are a fundamentally different approach.

### Getting started

```bash
git clone https://github.com/nearai/ironclaw.git
cd ironclaw

# Build
cargo build --release

# Set up PostgreSQL with pgvector
createdb ironclaw
psql ironclaw -c "CREATE EXTENSION IF NOT EXISTS vector;"

# Run the setup wizard (handles DB connection, NEAR AI auth, encryption)
ironclaw onboard

# Start as REPL
cargo run
```

You'll need Rust 1.85+, PostgreSQL 15+ with pgvector, and a NEAR AI account (the setup wizard handles the OAuth flow through your browser).

### Who this is for

IronClaw is for people who want the strongest security guarantees available in this space. The WASM sandbox and credential isolation make it the safest option for running an AI agent with real tool access on a production machine. The Rust codebase gives native performance and memory safety. The downsides: it's newer (368 stars), requires PostgreSQL infrastructure, and requires a NEAR AI auth requirement that may not sit well with everyone who wants a fully independent self-hosted setup.

## 7. ZeroClaw

<Button text="GitHub Repository" link="https://github.com/zeroclaw-labs/zeroclaw" variant="solid" color="blue" size="md" icon="github" />

ZeroClaw is a Rust-based assistant that pushes resource efficiency further than PicoClaw. The release binary is 3.4MB, it uses under 5MB of RAM, and it boots in under 10ms. The project comes from zeroclaw-labs and has 22+ built-in providers, 8+ chat channels, and a SQLite memory system with hybrid search (FTS5 keyword + vector cosine similarity). We wrote a [full ZeroClaw setup guide](/zeroclaw-setup-guide/) covering MiniMax M2.5, GLM-5, and Discord if you want to try it.

### What it does

- Single Rust binary, no runtime dependencies beyond the binary itself
- 8+ channels: CLI, Telegram, Discord, Slack, iMessage, Matrix, WhatsApp, Webhook
- 22+ LLM providers built in (OpenRouter, Anthropic, OpenAI, Ollama, Gemini, Groq, Mistral, xAI, DeepSeek, and more)
- SQLite hybrid memory: FTS5 keyword search + vector embeddings with cosine similarity
- Gateway pairing with 6-digit one-time codes and bearer token auth
- Workspace sandboxing, command allowlists, and forbidden path protection
- Encrypted secrets storage (ChaCha20-Poly1305)
- Docker support with distroless production images
- Built-in `zeroclaw migrate openclaw` command for switching from OpenClaw
- TOML configuration instead of JSON

### Security approach

ZeroClaw's security defaults are stricter than most projects on this list. The gateway binds to `127.0.0.1` and refuses to go public without a tunnel. Empty channel allowlists deny all messages by default (opposite of most bots). There's a 6-digit pairing code flow before the gateway accepts webhook requests.

```toml
[autonomy]
workspace_only = true
allowed_commands = ["git", "npm", "cargo", "ls", "cat", "grep"]
forbidden_paths = ["/etc", "/root", "/proc", "/sys", "~/.ssh", "~/.gnupg", "~/.aws"]

[secrets]
encrypt = true
```

14 system directories and 4 sensitive dotfiles are blocked by default. Symlink escape attempts get caught through path canonicalization.

### Getting started

```bash
git clone https://github.com/zeroclaw-labs/zeroclaw.git
cd zeroclaw
cargo build --release --locked
cargo install --path . --force --locked

# Interactive setup
zeroclaw onboard --interactive

# Chat
zeroclaw agent -m "Hello!"

# Start all channels
zeroclaw daemon
```

On a Raspberry Pi with 1GB RAM, use `CARGO_BUILD_JOBS=1 cargo build --release` to avoid the kernel killing rustc.

### Who this is for

ZeroClaw is the pick if you want the lowest possible resource footprint with serious security defaults and a wide range of built-in providers. The 22+ provider support means you can point it at nearly any LLM API without custom configuration. The SQLite hybrid memory is more capable than what PicoClaw or nanobot offer. The tradeoff is compile time (Rust builds take a few minutes on a VPS) and a newer, smaller community. If you're migrating from OpenClaw, the built-in migration command makes the switch straightforward.

## Which one should you pick?

It depends on what you actually need:

<ListCheck>
<ul>
<li>Want a tiny, readable, security-focused Telegram bot? Go with NanoClaw.</li>
<li>Need to be on Telegram, Discord, WhatsApp, and Slack at once? nanobot handles that.</li>
<li>Building your own agent and need a real memory system? memU is the memory framework to look at.</li>
<li>Want multi-agent teams on Discord? bitdoze-bot does that with Agno.</li>
<li>Running on extremely limited hardware or want a single Go binary with no dependencies? PicoClaw.</li>
<li>Need serious security isolation with WASM-sandboxed tools? IronClaw is the hardened option.</li>
<li>Want ultra-low resource usage (under 5MB RAM) with 22+ providers and SQLite hybrid memory? ZeroClaw.</li>
</ul>
</ListCheck>

All four run on a basic VPS. A Hetzner CX22 ($5.50/month) is enough for any of them. API costs depend on which model you pick and how much you chat, but $15-50/month covers most people.

If you haven't tried any self-hosted AI bot yet, I'd actually recommend starting with [OpenClaw itself](/clawdbot-setup-guide/). It's the most documented, has the largest community, and the setup wizard makes the first run pretty painless. Once you know what you want to change about it, these alternatives start making more sense.

<Accordion label="Frequently asked questions" group="faq" expanded="true">

**Can I switch from OpenClaw to one of these without losing my conversations?**

Not directly. Each project stores memory differently. You'd need to export from OpenClaw and manually import, or just start fresh. memU has the most flexible import options since it's designed as a memory framework.

**Do any of these work on a Raspberry Pi?**

NanoClaw and nanobot can run on a Pi 4 with 4GB+ RAM. Performance will be limited. memU with PostgreSQL needs more resources. bitdoze-bot depends on how many agents you run. PicoClaw is the clear winner here, running on boards as cheap as $10 with under 10MB of RAM. ZeroClaw is a close second with under 5MB RAM at runtime, though it needs more RAM during compilation. IronClaw needs PostgreSQL, so a Pi 4 with 4GB is the minimum.

**Can I use local models instead of API providers?**

nanobot has native vLLM support. NanoClaw and bitdoze-bot work with any OpenAI-compatible endpoint, so you can point them at Ollama or vLLM. memU supports custom LLM providers. PicoClaw works with OpenRouter and several direct providers. ZeroClaw has a built-in Ollama provider and supports any OpenAI-compatible endpoint via the `custom:` provider. IronClaw routes through NEAR AI. See our [Ollama Docker guide](/ollama-docker-install/) for setting up local models.

**How much coding do these require?**

NanoClaw and nanobot are close to zero-code for basic setups. PicoClaw is also minimal, just edit a JSON config and run a binary. ZeroClaw is similar, edit a TOML config and the interactive onboard wizard handles the rest. bitdoze-bot needs some YAML configuration for agents. memU requires Python integration work since it's a framework, not a standalone bot. IronClaw requires Rust tooling and PostgreSQL setup, but the onboard wizard handles most of the configuration.

</Accordion>

If you're settled on OpenClaw and want a proper UI for it, our [best OpenClaw dashboards](/best-openclaw-dashboards/) guide covers nine community-built options — from full multi-agent orchestration platforms like Mission Control down to lightweight single-file monitors. For getting started with OpenClaw itself, the [setup guide](/clawdbot-setup-guide/) has the full installation walkthrough.
