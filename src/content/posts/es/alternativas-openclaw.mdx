---
date: 2026-02-10T00:00:00Z
title: "Alternativas a OpenClaw Que Vale la Pena Probar en 2026"
description: "Una mirada a NanoClaw, nanobot, memU, bitdoze-bot, PicoClaw e IronClaw como alternativas self-hosted a OpenClaw para ejecutar tu propio asistente de IA 24/7."
image: "@assets/images/26/02/openclaw-alternatives.svg"
categories: ["AI"]
authors: ["Dragos"]
tags: ["ai-tools", "self-hosted"]
---

import Notice from "@components/widgets/Notice.astro";
import Button from "@components/widgets/Button.astro";
import ListCheck from "@components/widgets/ListCheck.astro";
import Accordion from "@components/widgets/Accordion.astro";
import YouTubeEmbed from "@components/widgets/YouTubeEmbed.astro";
import Tabs from "@components/widgets/Tabs.astro";
import Tab from "@components/widgets/Tab.astro";

OpenClaw (el proyecto que paso por los cambios de nombre de Clawdbot y Moltbot) hizo que mucha gente se diera cuenta de que podian ejecutar un asistente de IA en su propio servidor. Siempre encendido, siempre accesible a traves de Telegram o Slack, y no dependiente del SaaS de nadie. Lo he estado ejecutando yo mismo y escribi una [guia de configuracion completa](/es/guia-configuracion-openclaw/) si quieres probar el original.

Pero OpenClaw ya no es la unica opcion. Varios proyectos han aparecido con diferentes enfoques sobre la misma idea. Algunos son mas pequenos y enfocados. Algunos intentan hacer mas. He estado mirando cuatro de ellos, y cada uno hace compensaciones diferentes que vale la pena conocer.

<Notice type="info" title="Lo que cubre esto">
Seis proyectos de bots de IA self-hosted que funcionan como alternativas a OpenClaw. Cada seccion incluye que hace el proyecto, en que es diferente, y como hacerlo funcionar.
</Notice>

## Comparacion rapida

Antes de entrar en cada proyecto, aqui esta como se comparan:

| Feature | NanoClaw | nanobot | memU | bitdoze-bot | PicoClaw | IronClaw |
|---|---|---|---|---|---|---|
| **Estrellas GitHub** | ~10 | 15.4k | 8.7k | 7 | 1.1k | 368 |
| **Lenguaje** | Python | Python | Python + Rust | Python | Go | Rust |
| **Tamano del codigo** | ~3k lineas | ~3.5k lineas | Mas grande (framework) | Medio | Pequeno (binario unico) | Medio-grande |
| **Licencia** | MIT | MIT | Apache 2.0 | MIT | MIT | Apache 2.0 / MIT |
| **Canales de chat** | Telegram | Telegram, Discord, WhatsApp, Slack, Feishu, DingTalk, Email, QQ | Bot en memu.bot | Discord | Telegram, Discord | REPL, HTTP, Telegram, Slack (WASM) |
| **Memoria** | SQLite | Integrada | Jerarquica (feature principal) | Memoria Agno + aprendizaje | Basada en archivos workspace | PostgreSQL + pgvector (busqueda hibrida) |
| **Metodo de instalacion** | Docker / manual | pip (`nanobot-ai`) | pip (`memu-py`) | UV + manual | Binario unico / source | cargo build |
| **Modelos locales** | Via OpenRouter | Soporte vLLM | Via proveedores custom | Compatible OpenAI | Via OpenRouter | Via NEAR AI |
| **Features de seguridad** | FileGuard, ShellSandbox, PromptGuard | Basico | N/A (framework) | Permisos de herramientas, auditoria | Basico | Sandbox WASM, proteccion de credenciales, defensa inyeccion de prompts |
| **Multi-agente** | No | No | No (capa de memoria) | Si (equipos Agno) | No | Trabajos paralelos con workers aislados |

## 1. NanoClaw

<Button text="Repositorio GitHub" link="https://github.com/ysz/nanoClaw" variant="solid" color="green" size="md" icon="github" />

NanoClaw es el proyecto mas pequeno en esta lista. Alrededor de 3,000 lineas de Python, licencia MIT, solo Telegram. El autor lo construyo como un OpenClaw reducido que realmente puedes leer en una tarde.

Lo que llamo mi atencion es la capa de seguridad. La mayoria de estos bots le dan a la IA alguna forma de acceso a archivos y shell, y NanoClaw es el unico que construyo protecciones explicitas desde el inicio.

### Lo que hace

- Bot de Telegram que responde a mensajes y puede usar herramientas (acceso a archivos, comandos shell, busqueda web)
- Memoria basada en SQLite que persiste entre reinicios
- Dashboard web para monitorear conversaciones y memoria
- Agnostico de modelo a traves de OpenRouter, asi que eliges cualquier LLM que quieras
- Soporte Docker para despliegue

### Enfoque de seguridad

NanoClaw tiene tres mecanismos de seguridad integrados:

- **FileGuard**: Restringe de que directorios la IA puede leer y escribir. Defines rutas permitidas en configuracion.
- **ShellSandbox**: Los comandos shell se ejecutan en un entorno restringido. Sin situaciones de `rm -rf /`.
- **PromptGuard**: Filtra intentos de inyeccion de prompts antes de que lleguen al modelo.

Ninguno de estos es a prueba de balas, pero tenerlos pone a NanoClaw adelante de proyectos que simplemente le dan un shell a la IA y esperan lo mejor.

### Como empezar

<Tabs>
<Tab name="Docker">

```bash
git clone https://github.com/ysz/nanoClaw.git
cd nanoClaw
cp .env.example .env
# Edita .env con tu token de bot de Telegram y clave API de OpenRouter
docker compose up -d
```

</Tab>
<Tab name="Manual">

```bash
git clone https://github.com/ysz/nanoClaw.git
cd nanoClaw
pip install -r requirements.txt
cp .env.example .env
# Edita .env con tus tokens
python main.py
```

</Tab>
</Tabs>

### Para quien es

Si quieres algo lo suficientemente pequeno para entender completamente, y solo necesitas Telegram, NanoClaw es una opcion solida. Las features de seguridad hacen menos estresante darle acceso a herramientas a la IA. La desventaja es una comunidad minuscula (10 estrellas al momento de escribir) y soporte solo para Telegram.

## 2. nanobot

<Button text="Repositorio GitHub" link="https://github.com/HKUDS/nanobot" variant="solid" color="blue" size="md" icon="github" />

nanobot viene de HKUDS (Universidad de Hong Kong) y ha crecido rapido. 15,400 estrellas, 2,200 forks. El pitch es similar a NanoClaw pero mucho mas ambicioso: un bot ligero (~3,500 lineas de codigo) que se conecta a basicamente todas las plataformas de chat. Tenemos una [guia completa de configuracion de nanobot](/nanobot-setup-guide/) con MiniMax M2.5, GLM-5 y Brave Search si quieres probarlo.

### Lo que hace

- Se conecta a Telegram, Discord, WhatsApp, Slack, Feishu, DingTalk, Email y QQ
- Instalable via pip: `pip install nanobot-ai`
- Sistema de memoria integrado
- Soporta modelos locales a traves de vLLM
- Funciona con OpenRouter, Anthropic, OpenAI, DeepSeek, Groq, Gemini y otros
- Despliegue Docker disponible

La cobertura de canales es lo que distingue a nanobot. Si necesitas tu bot en WhatsApp y Discord y Slack al mismo tiempo, la mayoria de alternativas no hacen eso sin trabajo extra significativo.

### Configuracion

```bash
pip install nanobot-ai
nanobot init
# Sigue el asistente para configurar canales y claves API
nanobot start
```

Eso es realmente todo para una configuracion basica. El asistente `init` te guia eligiendo una plataforma de chat y conectando un proveedor LLM. Puedes agregar mas canales despues.

### Soporte de modelos locales

nanobot puede conectarse a un servidor vLLM local, lo que significa que puedes ejecutar todo el stack sin costos de API despues de la inversion inicial de hardware. Si ya tienes una configuracion Ollama o vLLM, apuntar nanobot a el es directo.

```bash
# Ejemplo: usando un endpoint vLLM local
nanobot config set llm.base_url http://localhost:8000/v1
nanobot config set llm.model tu-modelo-local
```

### Para quien es

nanobot es la opcion pragmatica si necesitas soporte multi-canal o quieres algo que puedes instalar con pip y tener funcionando en cinco minutos. El respaldo universitario y la gran comunidad significan que los bugs se arreglan y las features se agregan regularmente. La compensacion es que con tantas integraciones, la configuracion puede volverse densa. Nuestra [guia de nanobot](/nanobot-setup-guide/) cubre la configuracion paso a paso con proveedores como MiniMax M2.5 y GLM-5.

## 3. memU

<Button text="Repositorio GitHub" link="https://github.com/NevaMind-AI/MemU" variant="solid" color="purple" size="md" icon="github" />

memU es diferente de los otros proyectos aqui. No es realmente un chatbot. Es un framework de memoria construido para agentes 24/7, y resulta que incluye un bot (en memu.bot) como implementacion de referencia.

La idea central: la mayoria de los chatbots olvidan todo entre sesiones, e incluso los que tienen memoria solo hacen recuperacion basica. memU trata la memoria como un sistema de archivos, con categorias, items y referencias cruzadas, y trata de predecir lo que estas a punto de necesitar antes de que lo pidas.

### Como funciona la memoria

memU organiza todo en tres capas:

| Capa | Que almacena | Proposito |
|---|---|---|
| Recursos | Conversaciones crudas, documentos, imagenes | Datos originales |
| Items | Hechos extraidos, preferencias, skills | Conocimiento buscable |
| Categorias | Topicos auto-organizados | Navegacion y contexto |

La metafora de "sistema de archivos" significa que la memoria de tu agente se ve asi:

```
memory/
├── preferences/
│   ├── communication_style.md
│   └── topic_interests.md
├── knowledge/
│   ├── domain_expertise/
│   └── learned_skills/
└── context/
    ├── recent_conversations/
    └── pending_tasks/
```

Las memorias nuevas se auto-categorizan. Las memorias relacionadas se enlazan entre si. El sistema afirma 92% de precision en el benchmark Locomo, que prueba que tan bien los sistemas de memoria retienen y recuperan informacion en conversaciones largas.

### Comportamiento proactivo

Aqui es donde memU se vuelve interesante. En lugar de solo responder cuando se le pregunta, monitorea conversaciones y trata de anticipar lo que necesitaras despues. El agente puede:

- Pre-cargar contexto relevante antes de que lo pidas explicitamente
- Notar patrones en lo que estas trabajando y mostrar memorias relacionadas
- Borradores de items de accion del flujo de conversacion
- Aprender tus preferencias con el tiempo y ajustar respuestas

Si esto es util o molesto probablemente depende de tu tolerancia para sugerencias no solicitadas. Puedo ver funcionando bien para alguien que usa un asistente de IA todo el dia, menos para uso ocasional.

### Configuracion

<Tabs>
<Tab name="Cloud">

La version hosteada en [memu.so](https://memu.so) corre continuamente. Si quieres probar un sistema de memoria sin self-hosting, este es el camino mas rapido.

</Tab>
<Tab name="Self-hosted">

```bash
pip install memu-py

# Prueba en memoria (sin base de datos necesaria)
export OPENAI_API_KEY=tu_clave
cd tests
python test_inmemory.py

# Con PostgreSQL para almacenamiento persistente
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

python test_postgres.py
```

</Tab>
</Tabs>

memU tambien soporta OpenRouter, asi que puedes enrutar a traves del proveedor de modelo que prefieras:

```python
from memu import MemoryService

service = MemoryService(
    llm_profiles={
        "default": {
            "provider": "openrouter",
            "base_url": "https://openrouter.ai",
            "api_key": "tu_clave_api_openrouter",
            "chat_model": "anthropic/claude-3.5-sonnet",
        },
    },
)
```

### Para quien es

memU tiene mas sentido si estas construyendo tu propio agente y quieres una capa de memoria apropiada debajo. Tambien vale la pena mirarlo si estas frustrado con que tan superficial es la memoria en otros bots. El proyecto tiene 8,700 estrellas y una comunidad activa. La desventaja es la complejidad. Este no es un bot de "clona y ejecuta" como NanoClaw. Es un framework, y necesitaras integrarlo en algo.

## 4. bitdoze-bot

<Button text="Repositorio GitHub" link="https://github.com/bitdoze/bitdoze_bot" variant="solid" color="red" size="md" icon="github" />

Este es mi proyecto. Lo construi porque queria un bot de Discord que pudiera manejar multiples agentes especializados trabajando juntos, no solo un modelo respondiendo preguntas.

bitdoze-bot usa el [framework Agno](/agno-get-start/) para orquestacion multi-agente. Defines "equipos" de agentes en carpetas de workspace, cada agente con sus propias herramientas y personalidad, y un coordinador enruta los mensajes entrantes al especialista correcto.

Escribi una guia de construccion detallada: [Construye tu propio bot de Discord con IA con equipos Agno](/create-your-own-ai-agent/).

<YouTubeEmbed
  url="https://www.youtube.com/embed/yoWGFO7tvpc"
  label="Construi un Asistente de IA Personal en 90 Minutos"
/>

### Lo que hace

- Discord primero (responde mencion)
- Equipos multi-agente: un coordinador + agentes especialistas (codificacion, investigacion, ops, lo que definas)
- Configuracion basada en workspace: cada agente obtiene su propia carpeta con instrucciones, herramientas y permisos
- Memoria y aprendizaje: almacena contexto entre conversaciones y aprende de correcciones
- Heartbeat + cron: verificaciones de salud programadas y tareas recurrentes
- Permisos de herramientas y logging de auditoria
- Observabilidad: logs estructurados para cada ejecucion de agente

### La configuracion de equipos

El enfoque multi-agente es la principal diferencia de todo lo demas en esta lista. En lugar de un modelo tratando de hacer todo, divides responsabilidades:

```
workspaces/
├── main/
│   └── agent.yaml          # Coordinador - enruta a especialistas
├── coding/
│   └── agent.yaml          # Especialista de codificacion
├── research/
│   └── agent.yaml          # Especialista de investigacion web
└── ops/
    └── agent.yaml          # Especialista de ops de servidor
```

Cuando entra un mensaje, el coordinador decide que especialista lo maneja. Puedes empezar con solo el agente `main` y agregar especialistas segun los necesites.

### Como empezar

```bash
git clone https://github.com/bitdoze/bitdoze_bot.git
cd bitdoze_bot
cp .env.example .env
# Edita .env con token de Discord y claves API

# Instala con UV
uv sync
uv run python main.py
```

Necesitas Python 3.12+, un token de bot de Discord, y una clave API para tu proveedor de modelo.

### Para quien es

Si quieres ir mas alla de un chatbot de agente unico y experimentar con coordinacion multi-agente, este es el proyecto a mirar. La configuracion basada en workspace hace facil agregar nuevos especialistas sin tocar el codigo central. La compensacion es que es solo Discord y el proyecto mas pequeno aqui por conteo de estrellas. Pero es uno que uso diariamente, y el enfoque multi-agente ha valido la configuracion extra.

## 5. PicoClaw

<Button text="Repositorio GitHub" link="https://github.com/sipeed/picoclaw" variant="solid" color="gray" size="md" icon="github" />

PicoClaw toma el enfoque opuesto de todo lo demas en esta lista. En lugar de agregar features, las elimina. El proyecto es una reescritura en Go de nanobot que compila a un binario unico, usa menos de 10MB de RAM, y arranca en menos de un segundo. Sipeed (la compania de hardware RISC-V) lo construyo para ejecutarse en sus placas LicheeRV-Nano de $10, lo que dice mucho sobre el presupuesto de recursos con el que trabajaban.

Todo supuestamente fue escrito en un solo dia, con el agente de IA mismo manejando la mayor parte de la migracion a Go. Eso suena como un truco, pero el resultado realmente funciona. El binario corre en RISC-V, ARM y x86 sin cambios.

### Lo que hace

- Asistente de IA de binario unico, sin dependencias de runtime
- Soporte Telegram y Discord
- Acceso a herramientas: comandos shell, operaciones de archivos, busqueda web (via Brave Search API)
- Funciona con OpenRouter, Zhipu, Anthropic, OpenAI, Gemini, Groq y DeepSeek
- Almacenamiento de archivos basado en workspace para memoria y logs
- Modo CLI para uso local, modo gateway para canales de chat
- Transcripcion de mensajes de voz a traves del Whisper de Groq

### Comparacion de recursos

Los numeros aqui son dificiles de ignorar:

| Metrica | OpenClaw | nanobot | PicoClaw |
|---|---|---|---|
| **RAM** | >1GB | >100MB | menos de 10MB |
| **Inicio (core 0.8GHz)** | >500s | >30s | menos de 1s |
| **Costo minimo de hardware** | Mac Mini $599 | ~$50 SBC | ~$10 placa |

Si tienes un NanoKVM o MaixCAM por ahi, PicoClaw puede convertirlo en un asistente siempre encendido. Ese es un caso de uso que ninguno de los otros proyectos aqui puede tocar.

### Como empezar

```bash
# Descarga binario precompilado de releases, o compila desde source:
git clone https://github.com/sipeed/picoclaw.git
cd picoclaw
make build

# Inicializa configuracion
picoclaw onboard

# Edita ~/.picoclaw/config.json con tus claves API

# Chatea directamente
picoclaw agent -m "Cuanto es 2+2?"

# O inicia como gateway para Telegram/Discord
picoclaw gateway
```

### Para quien es

PicoClaw es la opcion si te importa la eficiencia de recursos por encima de todo, o si quieres ejecutar un bot en hardware que se ahogaria con Python. El codigo Go es pequeno y legible, y el despliegue de binario unico significa que no hay nada que instalar. La compensacion es que es nuevo (lanzado Febrero 2026), asi que el conjunto de features es mas limitado que nanobot, y la comunidad todavia se esta formando. Pero 1,100 estrellas en unos pocos dias sugiere que la gente esta prestando atencion.

## 6. IronClaw

<Button text="Repositorio GitHub" link="https://github.com/nearai/ironclaw" variant="solid" color="blue" size="md" icon="github" />
<Button text="Fork ZeroClaw" link="https://github.com/theonlyhennygod/zeroclaw" variant="solid" color="blue" size="md" icon="github" />

IronClaw viene de NEAR AI y toma un enfoque security-first que va mucho mas alla de lo que los otros proyectos intentan. Es una reescritura completa en Rust del concepto de OpenClaw, y el principal punto de venta es el sandbox WASM. Cada herramienta no confiable se ejecuta dentro de un contenedor WebAssembly aislado con permisos basados en capacidades explicitas. Tus claves API nunca se exponen al codigo de herramientas. Las solicitudes HTTP solo van a hosts que has aprobado.

Si alguna vez te has sentido nervioso por darle acceso shell a un agente de IA en una caja con datos reales, IronClaw fue construido para esa ansiedad.

### Lo que hace

- Asistente de IA nativo en Rust con REPL, webhook HTTP y canales basados en WASM (Telegram, Slack)
- Sandbox WASM para toda ejecucion de herramientas con permisos basados en capacidades
- Proteccion de credenciales: los secretos se inyectan en el limite del host, el codigo de herramientas nunca los ve
- Defensa contra inyeccion de prompts con deteccion de patrones y sanitizacion de contenido
- Endpoint allowlisting para que el agente solo pueda alcanzar hosts que apruebas
- PostgreSQL con pgvector para memoria (busqueda hibrida texto completo + vector)
- Sistema heartbeat para tareas proactivas en segundo plano
- Ejecucion de trabajos paralelos con contextos aislados
- Auto-expansible: describe una herramienta que necesitas, e IronClaw la construye como modulo WASM
- Soporte de protocolo MCP para conectar servidores de herramientas externos

### El modelo de seguridad

Aqui es donde IronClaw se destaca. El pipeline de seguridad para ejecucion de herramientas se ve asi:

1. La solicitud de herramienta llega al validador de allowlist de endpoints
2. La solicitud se escanea por fugas de credenciales
3. Las credenciales se inyectan en el limite del host (el codigo de la herramienta nunca las tiene)
4. La solicitud se ejecuta dentro del sandbox WASM
5. La respuesta se escanea de nuevo por fugas de credenciales
6. El resultado retorna al agente

Tambien hay limites de tasa por herramienta y tapas de recursos (memoria, tiempo de CPU, duracion de ejecucion). Las reglas de politica te permiten establecer niveles de severidad para diferentes situaciones: bloquear, advertir, revisar o sanitizar.

Ningun otro proyecto en esta lista tiene nada cerca de este nivel de aislamiento. NanoClaw tiene su FileGuard y ShellSandbox, pero esos son protecciones a nivel Python. Los contenedores WASM de IronClaw son un enfoque fundamentalmente diferente.

### Como empezar

```bash
git clone https://github.com/nearai/ironclaw.git
cd ironclaw

# Compila
cargo build --release

# Configura PostgreSQL con pgvector
createdb ironclaw
psql ironclaw -c "CREATE EXTENSION IF NOT EXISTS vector;"

# Ejecuta el asistente de configuracion (maneja conexion DB, auth NEAR AI, encriptacion)
ironclaw onboard

# Inicia como REPL
cargo run
```

Necesitas Rust 1.85+, PostgreSQL 15+ con pgvector, y una cuenta NEAR AI (el asistente de configuracion maneja el flujo OAuth a traves de tu navegador).

### Para quien es

IronClaw es para personas que quieren las garantias de seguridad mas fuertes disponibles en este espacio. El sandbox WASM y el aislamiento de credenciales lo hacen la opcion mas segura para ejecutar un agente de IA con acceso real a herramientas en una maquina de produccion. El codigo Rust da rendimiento nativo y seguridad de memoria. Las desventajas: es mas nuevo (368 estrellas), requiere infraestructura PostgreSQL, y un requisito de auth NEAR AI que puede no sentarle bien a todos los que quieren una configuracion self-hosted completamente independiente.

## Cual deberias elegir?

Depende de que realmente necesitas:

<ListCheck>
<ul>
<li>Quieres un bot de Telegram pequeno, legible y enfocado en seguridad? Ve con NanoClaw.</li>
<li>Necesitas estar en Telegram, Discord, WhatsApp y Slack al mismo tiempo? nanobot maneja eso.</li>
<li>Construyendo tu propio agente y necesitas un sistema de memoria real? memU es el framework de memoria a mirar.</li>
<li>Quieres equipos multi-agente en Discord? bitdoze-bot hace eso con Agno.</li>
<li>Ejecutando en hardware extremadamente limitado o quieres un binario Go unico sin dependencias? PicoClaw.</li>
<li>Necesitas aislamiento de seguridad serio con herramientas en sandbox WASM? IronClaw es la opcion endurecida.</li>
</ul>
</ListCheck>

Todos cuatro corren en un VPS basico. Un Hetzner CX22 ($5.50/mes) es suficiente para cualquiera de ellos. Los costos de API dependen de que modelo elijas y cuanto chats, pero $15-50/mes cubre a la mayoria de la gente.

Si no has probado ningun bot de IA self-hosted, en realidad recomiendo empezar con [OpenClaw mismo](/es/guia-configuracion-openclaw/). Es el mas documentado, tiene la comunidad mas grande, y el asistente de configuracion hace la primera ejecucion bastante indolora. Una vez que sepas que quieres cambiar de el, estas alternativas empiezan a tener mas sentido.

<Accordion label="Preguntas frecuentes" group="faq" expanded="true">

**Puedo cambiar de OpenClaw a uno de estos sin perder mis conversaciones?**

No directamente. Cada proyecto almacena memoria diferente. Necesitarias exportar de OpenClaw e importar manualmente, o simplemente empezar de nuevo. memU tiene las opciones de importacion mas flexibles ya que esta disenado como framework de memoria.

**Alguno de estos funciona en una Raspberry Pi?**

NanoClaw y nanobot pueden correr en un Pi 4 con 4GB+ de RAM. El rendimiento sera limitado. memU con PostgreSQL necesita mas recursos. bitdoze-bot depende de cuantos agentes ejecutes. PicoClaw es el claro ganador aqui, corriendo en placas tan baratas como $10 con menos de 10MB de RAM. IronClaw necesita PostgreSQL, asi que un Pi 4 con 4GB es el minimo.

**Puedo usar modelos locales en lugar de proveedores de API?**

nanobot tiene soporte nativo vLLM. NanoClaw y bitdoze-bot funcionan con cualquier endpoint compatible OpenAI, asi que puedes apuntarlos a Ollama o vLLM. memU soporta proveedores LLM custom. PicoClaw funciona con OpenRouter y varios proveedores directos. IronClaw enruta a traves de NEAR AI. Ve nuestra [guia Docker Ollama](/ollama-docker-install/) para configurar modelos locales.

**Cuanta programacion requieren estos?**

NanoClaw y nanobot son casi zero-code para configuraciones basicas. PicoClaw tambien es minimal, solo edita una config JSON y ejecuta un binario. bitdoze-bot necesita algo de configuracion YAML para agentes. memU requiere trabajo de integracion Python ya que es un framework, no un bot standalone. IronClaw requiere tooling Rust y configuracion PostgreSQL, pero el asistente onboard maneja la mayor parte de la configuracion.

</Accordion>
